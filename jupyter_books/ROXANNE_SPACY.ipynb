{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "\n",
    "from __future__ import unicode_literals, print_function\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import spacy\n",
    "import os\n",
    "import re\n",
    "\n",
    "TRAIN_DATA = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_operator(x):\n",
    "    match = re.search(r'[@:|?()\\.,#{}¡!\\w]*(:?'+ x[\"empresas\"] +\n",
    "                      '|'+ x[\"empresas\"].lower() +\n",
    "                      '|'+ x[\"empresas\"].upper() + ')(?:\\w+)?', x[\"texto\"])\n",
    "    \n",
    "    operator = np.nan\n",
    "    if match is not None:\n",
    "        operator = match.group(0)\n",
    "    \n",
    "    return operator\n",
    "    \n",
    "def fetch_clean_data():\n",
    "    files = os.listdir(\"csv/tweets_raw/\")\n",
    "    df = pd.DataFrame()\n",
    "    for file in files:\n",
    "        print(\"FILE:\", file)\n",
    "        df_file = pd.read_csv(\"csv/tweets_raw/\" + file, sep = \"|\")\n",
    "        if len(df) > 0:\n",
    "            df = df.append(df_file, ignore_index=True)\n",
    "        else:\n",
    "            df = df_file   \n",
    "    \n",
    "    df[\"emp_extracted\"] = \"\"\n",
    "    df[\"empresas\"] = df[\"empresas\"].fillna('')\n",
    "    empresas = df[\"empresas\"].unique()\n",
    "    for empresa in empresas:\n",
    "        df.loc[df[\"emp_extracted\"] == \"\", \"emp_extracted\"] = df[\"texto\"].str.extract('(?P<emp_extracted>' +\n",
    "                                                                           str(empresa) +\n",
    "                                                                           '|' + str(empresa).upper() +\n",
    "                                                                           '|' + str(empresa).lower() +')', expand = True)\n",
    "        df[\"emp_extracted\"] = df[\"emp_extracted\"].fillna(\"\")\n",
    "    \n",
    "    df.loc[df[\"empresas\"] == \"\", \"empresas\"] = np.nan\n",
    "    df[\"conteo_retweets\"] = df[\"conteo_retweets\"].values.astype(int)\n",
    "    df.loc[df[\"conteo_retweets\"] <= 0, \"conteo_retweets\"] = 0\n",
    "    df = df.drop(columns=[\"fecha_creacion\", \"palabras_clave\", \"autor\", \"ciudades\"])\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    df = df[df[\"texto\"].str.find(\"�\") == -1]\n",
    "    df = df.sort_values(by = \"tipo_lugar\")\n",
    "    df = df.sort_values(by = \"conteo_retweets\")\n",
    "    df = df.drop_duplicates([\"texto\"], keep=\"last\")\n",
    "    df.loc[pd.isna(df[\"empresas\"]), \"empresas\"] = \"X°X°X°X°X\"\n",
    "    df[\"empresas\"] = df[[\"empresas\", \"texto\"]].apply(find_operator, axis = 1)\n",
    "    df[\"horas\"] = df[\"texto\"].str.extract(\n",
    "        r'(?P<horas>\\d+(?=.{0}[ ]{0,5}?[hH][oO][rR][aA][sS]?))', \n",
    "        expand = True\n",
    "    )\n",
    "    df[\"horas\"] = df[\"horas\"].fillna(0)\n",
    "    df[\"horas\"] = df[\"horas\"].astype(int)\n",
    "    df[\"dias\"] = df[\"texto\"].str.extract(\n",
    "        r'(?P<dias>\\d+(?=.{0}[ ]{0,5}?[dD][iíIÍ][aA][sS]?))', \n",
    "        expand = True\n",
    "    )\n",
    "    df[\"dias\"] = df[\"dias\"].fillna(0)\n",
    "    df[\"dias\"] = df[\"dias\"].astype(int)\n",
    "    df.loc[pd.isna(df[\"empresas\"]), \"empresas\"] = df[\"emp_extracted\"]\n",
    "    df = df.drop(columns=[\"emp_extracted\"])\n",
    "    \n",
    "    print(\"CLEANED DATA\", len(df), \"TWEETS\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_tranning_data():\n",
    "    df_tweets = fetch_clean_data()\n",
    "    df_tweets[\"lugar_start\"] = df_tweets[~pd.isna(df_tweets[\"lugar\"])].apply(\n",
    "        lambda x: int(x[\"texto\"].find(x[\"lugar\"])), \n",
    "        axis = 1\n",
    "    )\n",
    "    df_tweets[\"lugar_end\"] = df_tweets[\"lugar_start\"] + df_tweets[\"lugar\"].str.len()\n",
    "    df_tweets[\"empresa_start\"] = df_tweets[~pd.isna(df_tweets[\"empresas\"])].apply(\n",
    "        lambda x: int(x[\"texto\"].find(x[\"empresas\"])), \n",
    "        axis=1\n",
    "    )\n",
    "    df_tweets[\"empresa_end\"] = df_tweets[\"empresa_start\"] + df_tweets[\"empresas\"].str.len()\n",
    "    df_tweets[\"tipo_lugar_start\"] = df_tweets[~pd.isna(df_tweets[\"tipo_lugar\"])].apply(\n",
    "        lambda x: int(x[\"texto\"].find(x[\"tipo_lugar\"])), \n",
    "        axis=1\n",
    "    )\n",
    "    df_tweets[\"tipo_lugar_end\"] = df_tweets[\"tipo_lugar_start\"] + df_tweets[\"tipo_lugar\"].str.len()\n",
    "    \n",
    "    df_tweets.loc[df_tweets[\"empresas\"] == \"\", \"empresas\"] = np.nan\n",
    "    df_tweets = df_tweets[~pd.isna(df_tweets[\"empresas\"])]\n",
    "    df_tweets[\"tipo_lugar\"] = df_tweets[\"tipo_lugar\"].fillna(\"None\")\n",
    "    df_tweets[\"lugar\"] = df_tweets[\"lugar\"].fillna(\"None\")\n",
    "    \n",
    "    tweets_lugares = df_tweets.to_dict('records')\n",
    "    \n",
    "    for tweet in tweets_lugares:\n",
    "        if (tweet[\"lugar\"] != \"None\") and (tweet[\"tipo_lugar\"] != \"None\"):\n",
    "            #print(tweet, 'LUGAR Y TIPO LUGAR')\n",
    "            TRAIN_DATA.append((\n",
    "                tweet[\"texto\"], {\n",
    "                    'entities': [\n",
    "                        (int(tweet[\"lugar_start\"]), int(tweet[\"lugar_end\"]), 'LOC'),\n",
    "                        (int(tweet[\"empresa_start\"]), int(tweet[\"empresa_end\"]), 'EMP')\n",
    "                    ],\n",
    "                    'cats':{tweet[\"tipo_lugar\"].upper(): 0.9}\n",
    "                }\n",
    "            ))\n",
    "        elif (tweet[\"lugar\"] != \"None\") and (tweet[\"tipo_lugar\"] == \"None\"):\n",
    "            #print(tweet, 'SOLO LUGAR')\n",
    "            TRAIN_DATA.append((\n",
    "                tweet[\"texto\"], {\n",
    "                    'entities': [\n",
    "                        (int(tweet[\"lugar_start\"]), int(tweet[\"lugar_end\"]), 'LOC'),\n",
    "                        (int(tweet[\"empresa_start\"]), int(tweet[\"empresa_end\"]), 'EMP')\n",
    "                    ],\n",
    "                    'cats':{'COLOMBIA':1.0}\n",
    "                }\n",
    "            ))\n",
    "        else:\n",
    "            #print(tweet, 'NINGUNO')\n",
    "            TRAIN_DATA.append((\n",
    "                tweet[\"texto\"], {\n",
    "                    'entities': [\n",
    "                        (int(tweet[\"empresa_start\"]), int(tweet[\"empresa_end\"]), 'EMP')\n",
    "                    ],\n",
    "                    'cats':{'COLOMBIA':1.0}\n",
    "                }\n",
    "            ))\n",
    "    \n",
    "    print(\"CREATED TRAINING DATA\", len(TRAIN_DATA), \"TWEETS\")\n",
    "    return TRAIN_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model=\"roxanne_model/\", output_dir=\"roxanne_model\", n_iter=10):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # Carga un modelo existente\n",
    "        print(\"Carga modelo '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('es')  # Crea un modelo vacio\n",
    "        print(\"Crear modelo vacio 'en' model\")\n",
    "\n",
    "    # Crea linea de accion para reconocimiento de labels y entidades\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # Si existe, la obtiene para agregar los labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    # Agrega los labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # Excluye otras lineas de accion para solo usar NER\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # Unicamente entrene NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                nlp.update(\n",
    "                    [text],  # Texto\n",
    "                    [annotations],  # Anotaciones\n",
    "                    drop=0.5,  # Ignora la mitad para dificultar el aprendizaje\n",
    "                    sgd=optimizer,  # Actualiza los pesos\n",
    "                    losses=losses)\n",
    "            print(losses)\n",
    "            \n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: registro_continuo.csv\n",
      "FILE: tweets_entrenamiento.csv\n",
      "FILE: filtered_tweets.csv\n",
      "FILE: tweets_entrenamiento_2.csv\n",
      "CLEANED DATA 2366 TWEETS\n",
      "CREATED TRAINING DATA 1657 TWEETS\n",
      "Carga modelo 'roxanne_model/'\n",
      "{'ner': 669.2271837170861}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7adb9f55be5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcreate_tranning_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-6567177799fd>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, output_dir, n_iter)\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Ignora la mitad para dificultar el aprendizaje\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Actualiza los pesos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     losses=losses)\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, docs, golds, drop, sgd, losses)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m                 \u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_gold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs_golds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32moptimizers.pyx\u001b[0m in \u001b[0;36mthinc.neural.optimizers.Optimizer.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mops.pyx\u001b[0m in \u001b[0;36mthinc.neural.ops.Ops.clip_gradient\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/thinc/neural/util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcupy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mget_array_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    create_tranning_data()\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model=\"roxanne_model/\"):\n",
    "    # Guarda el modelo en el directorio especificado\n",
    "    # Prueba el modelo creado\n",
    "    print(\"Loading from\", model)\n",
    "    nlp2 = spacy.load(model)\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp2(text)\n",
    "        print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('Chinga Tu Madre', 'LOC')]\n",
      "Tokens [('sin', '', 2), ('luz', '', 2), ('en', '', 2), ('el', '', 2), ('sector', '', 2), ('de', '', 2), ('Chinga', 'LOC', 3), ('Tu', 'LOC', 1), ('Madre', 'LOC', 1), (',', '', 2), ('@Electricaribe', '', 2), ('algún', '', 2), ('daño', '', 2), ('reportado', '', 2), ('?', '', 2), ('gracias', '', 2)]\n"
     ]
    }
   ],
   "source": [
    "nlp2 = spacy.load(\"roxanne_model/\")\n",
    "TRAIN_DATA = [\"sin luz en el sector de Chinga Tu Madre, @Electricaribe algún daño reportado? gracias\"]\n",
    "\n",
    "for text in TRAIN_DATA:\n",
    "    doc = nlp2(text)\n",
    "    print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "    print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
